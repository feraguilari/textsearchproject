{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac789e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46236b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load file from milestone 1 with tokens\n",
    "token_file = open(\"token_data.json\",'r')\n",
    "token_data = json.load(token_file)\n",
    "token_file.close()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51922a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create global token count and corpus vocab\n",
    "bag_of_words=[]\n",
    "for item in token_data:\n",
    "    bag_of_words.extend(item['tokenized_text'])\n",
    "    \n",
    "word_count= Counter(bag_of_words)\n",
    "corpus_vocab = list(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c6a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of articles that include a token\n",
    "token_article_count={}\n",
    "for token in word_count:\n",
    "    doc_count=sum([1 for doc in token_data if token in doc['tokenized_text']])\n",
    "    token_article_count[token]=doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4d49b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute tf_idf vector for each document\n",
    "idf ={token: np.log(len(token_data)/token_article_count[token]) for token in corpus_vocab}\n",
    "\n",
    "for article in token_data:\n",
    "    token_count = Counter(article['tokenized_text'])\n",
    "    token_total = len(article['tokenized_text'])\n",
    "    \n",
    "    tfidf_vec=[]\n",
    "    for token in corpus_vocab:\n",
    "        tf = token_count[token]/token_total\n",
    "        tfidf = tf * idf[token]\n",
    "        tfidf_vec.append(tfidf)\n",
    "    \n",
    "    article['tf_idf']=tfidf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e91268",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_data.json', 'w') as outfile:\n",
    "    json.dump(token_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c2cf4",
   "metadata": {},
   "source": [
    "### Query to Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf466760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text_lower = nlp(text.lower())\n",
    "    clean_tokens = [token.lemma_ for token in text_lower\n",
    "                   if not token.is_stop\n",
    "                   and not token.is_punct\n",
    "                   and len(token.dep_.strip())!=0]\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb6b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(query, vocab = corpus_vocab):\n",
    "    tokenized_query = tokenizer(query)\n",
    "    query_token_count = Counter(tokenized_query)\n",
    "    idf = {token: np.log(len(token_data) /  token_article_count[token]) for token in vocab}\n",
    "    \n",
    "    query_vec=[]\n",
    "    for token in vocab:\n",
    "        tf = query_token_count[token] / len(tokenized_query)\n",
    "        tfidf = tf * idf[token]\n",
    "        query_vec.append(tfidf)\n",
    "            \n",
    "    return query_vec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274d4ca",
   "metadata": {},
   "source": [
    "### Article Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e0191f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def art_search (query, articles):\n",
    "    query_vec = vectorize (query)\n",
    "    query_arr = np.array(query_vec)\n",
    "    \n",
    "    rankings = []\n",
    "    for art in articles:\n",
    "        art_rank={}\n",
    "        art_arr=np.array(art['tf_idf'])\n",
    "        rank = cosine_similarity(query_arr.reshape(1,-1), art_arr.reshape(1,-1))[0][0]\n",
    "        if rank > 0:\n",
    "            art_rank['title']=art['title']\n",
    "            art_rank['rank']=rank\n",
    "            rankings.append(art_rank)\n",
    "            \n",
    "    return sorted(rankings, key=lambda k: k['rank'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43dfdd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Pandemic', 'rank': 0.19377592595564383},\n",
       " {'title': 'Epidemiology of HIV/AIDS', 'rank': 0.119507273981255},\n",
       " {'title': 'HIV/AIDS in Yunnan', 'rank': 0.11473600135755814},\n",
       " {'title': 'HIV/AIDS', 'rank': 0.03836578720617653},\n",
       " {'title': 'Swine influenza', 'rank': 0.03593161240096674},\n",
       " {'title': 'COVID-19 pandemic', 'rank': 0.024757165823364867},\n",
       " {'title': 'Viral load', 'rank': 0.02146143400714939},\n",
       " {'title': 'Spanish flu', 'rank': 0.021248318942639555},\n",
       " {'title': 'Basic reproduction number', 'rank': 0.020925349281131524},\n",
       " {'title': 'Cholera', 'rank': 0.016536575925365078},\n",
       " {'title': '1929â€“1930 psittacosis pandemic', 'rank': 0.014327420984000376}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_search ('people', token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290d43f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
